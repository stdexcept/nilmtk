{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Pipeline(object):\n",
      "    \"\"\"\n",
      "    A data processing pipeline for processing power data.  Operates at\n",
      "    the \"Meter\" layer (Assuming that we have multiple layers of abstraction:\n",
      "    Data Source, Meter, Appliance, [ApplianceGroup?], \n",
      "    Building, [BuildingGroup?], DataSet)\n",
      "    \n",
      "    SOURCE -> LOADER/SPLITTER -> NODE_1 -> ... -> NODE_N\n",
      "    \n",
      "    A pipeline consists of one loader/splitter\n",
      "    node which loads and, if necessary, splits the data into chunks;\n",
      "    if there are K chunks then the pipeline runs K times; and on each\n",
      "    iteration the output from the loader/splitter is a single DataFrame\n",
      "    (with metatdata).\n",
      "    \n",
      "    The Loader contains a Source object which defines how to pull\n",
      "    data from the physical data store (disk / network / device).\n",
      "    \n",
      "    After the loader/splitter are an arbitrary number of \"nodes\"\n",
      "    which process data in sequence or export the data to disk.\n",
      "    \n",
      "    Each processing node has a set of preconditions (e.g. gaps must be\n",
      "    filled) and a set of postconditions (e.g. gaps will have been\n",
      "    filled).  This allows us to check that a particular pipeline is\n",
      "    viable (i.e. that, for every node, the node's preconditions are\n",
      "    satisfied by an upstream node or by the source).\n",
      "    \n",
      "    During a single cycle of the pipeline, results from each\n",
      "    stats node are stored in the `dataframe.results` dict.  At the end\n",
      "    of each pipeline cycle, the contents of dataframe.results \n",
      "    are combined and the aggregate results are stored in the pipeline.\n",
      "    \n",
      "    IDEAS FOR THE FUTURE???:\n",
      "    Pipelines could be saved/loaded from disk.\n",
      "    \n",
      "    If the pipeline was represented by a directed acyclic\n",
      "    graphical model (DAG) then:\n",
      "      pipeline could fork into multiple parallel\n",
      "      pipelines.  Data and metadata would be copied to each fork and\n",
      "      each sub-pipeline would be run as a separate process (after\n",
      "      checking requirements for each subpipeline as the start).\n",
      "    \n",
      "      Pipelines could be rendered\n",
      "      graphically.  In the future it would be nice to have a full\n",
      "      graphical UI (like Node-RED).\n",
      "    \n",
      "    Attributes\n",
      "    ----------\n",
      "    nodes : list of Node objects\n",
      "    loader : Loader\n",
      "    results : dict of Results objects storing aggregate stats results\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    >>> table_path = 'building1/utility/electric/meter1'\n",
      "    >>> source = HDFTableSource('ukpd.h5', table_path)\n",
      "    >>> loader = Loader(source, start=\"2013-01-01\", end=\"2013-06-01\")\n",
      "\n",
      "    Calculate total energy and save the preprocessed data\n",
      "    and the energy data back to disk:\n",
      "    \n",
      "    >>> nodes = [BookendGapsWithZeros(), \n",
      "                 Energy(), \n",
      "                 HDFTableExport('meter1_preprocessed.h5', table_path)]\n",
      "    >>> pipeline = Pipeline(loader, nodes).run()\n",
      "    >>> energy = pipeline.results['energy']\n",
      "    >>> print(\"Energy in Joules =\", energy.joules, \"and kWh =\", energy.kwh)\n",
      "    \n",
      "    \"\"\"\n",
      "    def __init__(self, loader=None, nodes=None):\n",
      "        self.loader = loader\n",
      "        self.nodes = nodes\n",
      "    \n",
      "    def run(self):\n",
      "        self.reset()\n",
      "        self.check_preconditions()\n",
      "        \n",
      "        # Run pipeline\n",
      "        for chunk in self.source.load(conditions.measurements()):\n",
      "            processed_chunk = self._run_chunk_through_pipeline(chunk)\n",
      "            self._update_aggregate_results(processed_chunk.results)\n",
      "\n",
      "    def _run_chunk_through_pipeline(self, chunk):\n",
      "        for node in nodes:\n",
      "            chunk = node.process(chunk)\n",
      "        return chunk\n",
      "    \n",
      "    def _update_aggregate_results(self, results_for_chunk):\n",
      "        for statistic, result in results_for_chunk.iteritems():\n",
      "            try:\n",
      "                self.results[statistic].update(result)\n",
      "            except KeyError:\n",
      "                self.results[statistic] = result\n",
      "                    \n",
      "    def reset(self):\n",
      "        self.results = {}\n",
      "        for node in self.nodes:\n",
      "            node.reset()\n",
      "            \n",
      "    def check_preconditions(self):\n",
      "        assert(isinstance(self.source, Source))\n",
      "        assert(isinstance(self.nodes, list))\n",
      "        assert(len(self.nodes) > 0))\n",
      "        \n",
      "        # Check requirements\n",
      "        condition = self.source.condition()\n",
      "\n",
      "        # for example, `condition` might represent:\n",
      "        # * [measurements] are available\n",
      "        # * gaps = Gaps([(\"2013-01-01 00:00\", \"2013-01-01 00:10\"), ...])\n",
      "        # * zeros have been inserted\n",
      "        # * sample_period = 1 second (I was thinking of putting this\n",
      "        #   in a separate, imutable 'metadata' field but sample_period\n",
      "        #   could be changed during the pipeline by an\n",
      "        #   up/down-sampling node)\n",
      "\n",
      "        for node in self.nodes: # go through the nodes in order, starting upstream\n",
      "            unsatisfied_preconditions = condition.unsatisfied_preconditions(node.preconditions)\n",
      "            if unsatisfied_preconditions:\n",
      "                raise Exception(str(unsatisfied_preconsitions))\n",
      "                # TODO: should explain exactly why preconditions aren't satisfied\n",
      "                # e.g. which node is unsatisfied? Precisely which preconditions aren't met?\n",
      "            condition.update(node.postconditions)\n",
      "            # for example, `condition` might now include:\n",
      "            # * total energy for this chunk has been calculated\n",
      "            # * from [measurements] available, we must load reactive power and active power\n",
      "\n",
      "\n",
      "class Source(object):\n",
      "    def load(self, measurements):\n",
      "        \"\"\"Returns a generator.  Each item returned by the generator\n",
      "        is a pd.DataFrame with metadata attached (sample period, gaps,\n",
      "        gaps_bookended_with_zeros etc).\"\"\"\n",
      "    \n",
      "    def condition(self):\n",
      "        return Condition()\n",
      "        \n",
      "\n",
      "class Condition(object):\n",
      "    \"\"\"Stores pre-conditions and post-conditions.\n",
      "    \n",
      "    Requirements can be of the form:\n",
      "    \n",
      "    \"node X needs (power.apparent or power.active) (but not\n",
      "    power.reactive) and voltage is useful but not essential\"\n",
      "    \n",
      "    or\n",
      "    \n",
      "    \"node Y needs everything available from disk (to save to a copy to\n",
      "    disk)\"\n",
      "    \n",
      "    or\n",
      "    \n",
      "    \"ComputeEnergy node needs gaps to be bookended with zeros\" (if\n",
      "    none of the previous nodes provide this service then check\n",
      "    source.metadata to see if zeros have already been inserted; if the\n",
      "    haven't then raise an error to tell the user to add a\n",
      "    BookendGapsWithZeros node.)\n",
      "    \"\"\"\n",
      "\n",
      "    def update(self, postconditions):\n",
      "        \"\"\"Add postconditions to self.\"\"\"\n",
      "        pass\n",
      "    \n",
      "    def unsatisfied_preconditions(self, preconditions):\n",
      "        \"\"\"Returns list of preconditions not satisfied by self.\"\"\"\n",
      "        \n",
      "\n",
      "class Node(object):\n",
      "    \"\"\"Abstract class defining interface for all Node subclasses,\n",
      "    where a 'node' is a module which runs pre-processing or statistics\n",
      "    or NILM training or disaggregation.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "    @classmethod\n",
      "    def preconditions(self):\n",
      "        return preconditions\n",
      "    \n",
      "    @classmethod\n",
      "    def postconditions(self):\n",
      "        return postconditions\n",
      "    \n",
      "    def process(self, df):\n",
      "        # do stuff to df\n",
      "        return df\n",
      "    \n",
      "#-------------- RESULT CLASSES -----------------#\n",
      "\n",
      "\"\"\"Metadata results from each node need to be assigned to a specific\n",
      "class so we know how to combine results from multiple chunks.  For\n",
      "example, Energy can be simply summed; while dropout rate should be\n",
      "averaged, and gaps need to be merged across chunk boundaries.  Results\n",
      "are a subclass of DataFrame.  The index is the start timestamp for\n",
      "which the results are valid; the first column ('end') is the end\n",
      "timestamp for which the results are valid.  Other columns are the\n",
      "results.  \"\"\"\n",
      "\n",
      "class Results(pd.DataFrame):\n",
      "    def combined(self):\n",
      "        \"\"\"Return all results from each chunk combined\"\"\"\n",
      "\n",
      "class EnergyResults(Results):\n",
      "    def combined(self):\n",
      "        return self.sum()\n",
      "\n",
      "#-------------- STATS NODES --------------------#\n",
      "    \n",
      "class StatsNode(Object):\n",
      "    \"\"\"\n",
      "    Abstract class for nodes which process statistics\n",
      "    \"\"\"\n",
      "    def results(self):\n",
      "        pass\n",
      "\n",
      "\n",
      "class LocateGapsNode(Node, StatsNode):\n",
      "    \n",
      "    @classmethod\n",
      "    def preconditions(self):\n",
      "        # Needs sample period to be set\n",
      "        # must be followed by a buffer node or an export node\n",
      "        \n",
      "    @classmethod\n",
      "    def postconditions(self):\n",
      "        # Gaps will be identified!\n",
      "\n",
      "class EnergyNode(Node, StatsNode):\n",
      "    \"\"\"Computes energy\"\"\"\n",
      "\n",
      "    name = 'energy'\n",
      "\n",
      "    @classmethod\n",
      "    def preconditions(self):\n",
      "        # Needs any power or energy measurements.  Preference is for\n",
      "        # energy measurements.  Computes energy for each of {active,\n",
      "        # reactive, apparent} available If only power measurements are\n",
      "        # available and if there are gaps then requires zeros to be\n",
      "        # inserted before gaps.\n",
      "        \n",
      "    @classmethod\n",
      "    def postconditions(self):\n",
      "        # Energy is calculated\n",
      "        pass\n",
      "           \n",
      "    def process(self, df):\n",
      "        # TODO: calculate energy_for_df\n",
      "        df.results[(self.name, self.instance)] = EnergyResults(energy_for_df)\n",
      "        return df\n",
      "    \n",
      "    def results(self):\n",
      "        return self._cumulator\n",
      "\n",
      "class ProportionEnergySubmeteredNode(Node, StatsNode):\n",
      "    def preconditions(self):\n",
      "        \"\"\"\n",
      "        * use the gaps in mains as a mask\n",
      "        * then calculate energy\n",
      "        \"\"\"\n",
      "\n",
      "#---------------- PROCESSING NODES --------------------#\n",
      "\n",
      "class ProcessingNode(Object):\n",
      "    \"\"\"\n",
      "    Abstract class for nodes which process data\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "class BookendGapsWithZerosNode(Node, ProcessingNode):\n",
      "    \n",
      "    def preconditions(self):\n",
      "        \"\"\"Requires gaps to be located.\"\"\"\n",
      "        \n",
      "    def postconditions(self):\n",
      "        \"\"\"Bookends gaps with zeros\"\"\"\n",
      "    \n",
      "    def process(self, df):\n",
      "        for gap_start, gap_end in df.metadata.gaps:\n",
      "            # insert zeros!\n",
      "            \n",
      "        return df\n",
      "    \n",
      "#--------------- EXPORT NODES --------------------#\n",
      "\n",
      "class ExportNode(Object):\n",
      "    \"\"\"\n",
      "    Abstract class for nodes which export data to disk / network etc\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "#--------------- BUFFER NODES ---------------------#\n",
      "\n",
      "class BufferNode(Node):\n",
      "    \"\"\"BufferNodes solve the following problem. Say we need to run the\n",
      "    chain on a dataset too large to fit in memory and we have a\n",
      "    processing chain like this:\n",
      "    \n",
      "    Source -> find gaps -> split on gaps -> calculate dropout rate\n",
      "    \n",
      "    In this case, we only have full knowledge of where the gaps are\n",
      "    once we have looked at all chunks.  A BufferNode forces the\n",
      "    Pipeline to run all chunks through the pipeline up to the Buffer\n",
      "    (and \"finish\" all metadata) before proceeding.\n",
      "    \n",
      "    Source -> find gaps -> BufferNode -> split on gaps -> calculate dropout rate\n",
      "    \n",
      "    BufferNode passed just the metadata onto the next stage.\n",
      "    \n",
      "    QUESTION: BufferNodes will always be proceeded by splitter nodes\n",
      "    so maybe they should be combined?\n",
      "    \n",
      "    QUESTION: Splitter nodes will need direct access to the Source.\n",
      "    How to implement this?  Maybe pass in a ref to Source to all\n",
      "    Nodes???\n",
      "    \n",
      "    QUESTION: maybe this is over complex???  Maybe we should force\n",
      "    users to first run\n",
      "    \n",
      "    Source('a.h5') -> find gaps -> export('a.h5')\n",
      "    \n",
      "    And then do\n",
      "    \n",
      "    Source('a.h5') -> split on gaps -> ...\n",
      "    \"\"\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}