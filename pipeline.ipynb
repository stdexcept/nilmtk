{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Pipeline(object):\n",
      "    \"\"\"\n",
      "    A data processing pipeline for processing power data.  Operates at\n",
      "    the \"Meter\" layer (Assuming that we have multiple layers of abstraction:\n",
      "    Data Source, Meter, Appliance, [ApplianceGroup?], \n",
      "    Building, [BuildingGroup?], DataSet)\n",
      "    \n",
      "    SOURCE -> LOADER/SPLITTER -> NODE_1 -> ... -> NODE_N\n",
      "    \n",
      "    A pipeline consists of one loader/splitter\n",
      "    node which loads and, if necessary, splits the data into chunks;\n",
      "    if there are K chunks then the pipeline runs K times; and on each\n",
      "    iteration the output from the loader/splitter is a single DataFrame\n",
      "    (with metatdata).\n",
      "    \n",
      "    The Loader contains a Source object which defines how to pull\n",
      "    data from the physical data store (disk / network / device).\n",
      "    \n",
      "    After the loader/splitter are an arbitrary number of \"nodes\"\n",
      "    which process data in sequence or export the data to disk.\n",
      "    \n",
      "    Each processing node has a set of preconditions (e.g. gaps must be\n",
      "    filled) and a set of postconditions (e.g. gaps will have been\n",
      "    filled).  This allows us to check that a particular pipeline is\n",
      "    viable (i.e. that, for every node, the node's preconditions are\n",
      "    satisfied by an upstream node or by the source).\n",
      "    \n",
      "    During a single cycle of the pipeline, results from each\n",
      "    stats node are stored in the `dataframe.results` dict.  At the end\n",
      "    of each pipeline cycle, the contents of dataframe.results \n",
      "    are combined and the aggregate results are stored in the pipeline.\n",
      "    \n",
      "    IDEAS FOR THE FUTURE???:\n",
      "    Pipelines could be saved/loaded from disk.\n",
      "    \n",
      "    If the pipeline was represented by a directed acyclic\n",
      "    graphical model (DAG) then:\n",
      "      pipeline could fork into multiple parallel\n",
      "      pipelines.  Data and metadata would be copied to each fork and\n",
      "      each sub-pipeline would be run as a separate process (after\n",
      "      checking requirements for each subpipeline as the start).\n",
      "    \n",
      "      Pipelines could be rendered\n",
      "      graphically.  In the future it would be nice to have a full\n",
      "      graphical UI (like Node-RED).\n",
      "    \n",
      "    Attributes\n",
      "    ----------\n",
      "    nodes : list of Node objects\n",
      "    loader : Loader\n",
      "    results : dict of Results objects storing aggregate stats results\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    >>> table_path = 'building1/utility/electric/meter1'\n",
      "    >>> source = HDFTableSource('ukpd.h5', table_path)\n",
      "    >>> loader = Loader(source, start=\"2013-01-01\", end=\"2013-06-01\")\n",
      "\n",
      "    Calculate total energy and save the preprocessed data\n",
      "    and the energy data back to disk:\n",
      "    \n",
      "    >>> nodes = [BookendGapsWithZeros(), \n",
      "                 Energy(), \n",
      "                 HDFTableExport('meter1_preprocessed.h5', table_path)]\n",
      "    >>> pipeline = Pipeline(loader, nodes).run()\n",
      "    >>> energy = pipeline.results['energy']\n",
      "    >>> print(\"Energy in Joules =\", energy.joules, \"and kWh =\", energy.kwh)\n",
      "    \n",
      "    \"\"\"\n",
      "    def __init__(self, loader=None, nodes=None):\n",
      "        self.loader = loader\n",
      "        self.nodes = nodes\n",
      "    \n",
      "    def run(self):\n",
      "        self.reset()\n",
      "        self.check_preconditions()\n",
      "        # Run pipeline\n",
      "        for chunk in self.source.load(conditions.measurements()):\n",
      "            processed_chunk = self._run_chunk_through_pipeline(chunk)\n",
      "            self._update_results(processed_chunk.results)\n",
      "\n",
      "    def _run_chunk_through_pipeline(self, chunk):\n",
      "        for node in nodes:\n",
      "            chunk = node.process(chunk)\n",
      "        return chunk\n",
      "    \n",
      "    def _update_results(self, results_for_chunk):\n",
      "        for statistic, result in results_for_chunk.iteritems():\n",
      "            try:\n",
      "                self.results[statistic].update(result)\n",
      "            except KeyError:\n",
      "                self.results[statistic] = result\n",
      "                    \n",
      "    def reset(self):\n",
      "        self.results = {}\n",
      "        for node in self.nodes:\n",
      "            node.reset()\n",
      "            \n",
      "    def check_preconditions(self):\n",
      "        assert(isinstance(self.source, Source))\n",
      "        assert(isinstance(self.nodes, list))\n",
      "        assert(len(self.nodes) > 0))\n",
      "        \n",
      "        # Check requirements\n",
      "        condition = self.source.metadata\n",
      "\n",
      "        # for example, `condition` might represent:\n",
      "        # * [measurements] are available\n",
      "        # * gaps = Gaps([(\"2013-01-01 00:00\", \"2013-01-01 00:10\"), ...])\n",
      "        # * zeros have been inserted\n",
      "        # * sample_period = 1 second (I was thinking of putting this\n",
      "        #   in a separate, imutable 'metadata' field but sample_period\n",
      "        #   could be changed during the pipeline by an\n",
      "        #   up/down-sampling node)\n",
      "\n",
      "        for node in self.nodes: # go through the nodes in order, starting upstream\n",
      "            node.check_preconditions(condition)\n",
      "            condition.update(node.postconditions)\n",
      "            # for example, `condition` might now include:\n",
      "            # * total energy for this chunk has been calculated\n",
      "            # * from [measurements] available, we must load reactive power and active power\n",
      "\n",
      "\n",
      "class Source(object):\n",
      "    \"\"\"\n",
      "    Attributes\n",
      "    ----------\n",
      "    metadata : dict\n",
      "    \"\"\"\n",
      "    \n",
      "    def load(self, measurements):\n",
      "        \"\"\"Returns a generator.  Each item returned by the generator\n",
      "        is a pd.DataFrame with metadata attached (sample period, gaps,\n",
      "        gaps_bookended_with_zeros etc).\"\"\"\n",
      "        \n",
      "\n",
      "class Node(object):\n",
      "    \"\"\"Abstract class defining interface for all Node subclasses,\n",
      "    where a 'node' is a module which runs pre-processing or statistics\n",
      "    or NILM training or disaggregation.\n",
      "    \"\"\"\n",
      "\n",
      "    postconditions =  {} # TODO\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "    @staticmethod\n",
      "    def check_preconditions(conditions):\n",
      "        \"\"\"\n",
      "        Parameters\n",
      "        ----------\n",
      "        conditions : dict\n",
      "        \n",
      "        Requirements can be of the form:\n",
      "    \n",
      "        \"node X needs (power.apparent or power.active) (but not\n",
      "        power.reactive) and voltage is useful but not essential\"\n",
      "    \n",
      "        or\n",
      "    \n",
      "        \"node Y needs everything available from disk (to save to a copy to\n",
      "        disk)\"\n",
      "    \n",
      "        or\n",
      "    \n",
      "        \"ComputeEnergy node needs gaps to be bookended with zeros\" (if\n",
      "        none of the previous nodes provide this service then check\n",
      "        source.metadata to see if zeros have already been inserted; if the\n",
      "        haven't then raise an error to tell the user to add a\n",
      "        BookendGapsWithZeros node.)\n",
      "        \"\"\"\n",
      "        # TODO: see if there are any unsatisfied preconditions\n",
      "        # if there are then raise an UnsatisfiedPreconditionsError\n",
      "        # giving the exact precondition that failed, why it failed\n",
      "        # which node is complaining, and suggestions for how to fix it\n",
      "        pass\n",
      "        \n",
      "    def process(self, df):\n",
      "        # check_preconditions again??? (in case this node is not run in\n",
      "        # the context of a Pipeline?)\n",
      "        # do stuff to df\n",
      "        return df\n",
      "    \n",
      "#-------------- RESULT CLASSES -----------------#\n",
      "\n",
      "class Results(pd.DataFrame):\n",
      "    \"\"\"Metadata results from each node need to be assigned to a specific\n",
      "    class so we know how to combine results from multiple chunks.  For\n",
      "    example, Energy can be simply summed; while dropout rate should be\n",
      "    averaged, and gaps need to be merged across chunk boundaries.  Results\n",
      "    objects contain a DataFrame, the index of which is the start timestamp for\n",
      "    which the results are valid; the first column ('end') is the end\n",
      "    timestamp for which the results are valid.  Other columns are accumaltors for the results.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    _accumulator : DataFrame\n",
      "        Index is period start.  Columns are: end_date and any columns for internal storage of stats.\n",
      "    \"\"\"\n",
      "    \n",
      "    @property\n",
      "    def combined(self):\n",
      "        \"\"\"Return all results from each chunk combined.\n",
      "        Either return single float for all periods or a dict where necessary, \n",
      "        e.g. if calculating Energy for a meter which records both apparent power and\n",
      "        active power then we've have energyresults.combined['active']\n",
      "        \"\"\"\n",
      "        pass\n",
      "        \n",
      "    @property\n",
      "    def per_period(self):\n",
      "        \"\"\"return a DataFrame.  Index is period start.  Columns are: end_date and <stat name>\n",
      "        \"\"\"\n",
      "    \n",
      "    def update(self, new_result):\n",
      "        \"\"\"Update with new results\"\"\"\n",
      "        pass\n",
      "\n",
      "\n",
      "class EnergyResults(Results):\n",
      "    @property\n",
      "    def combined(self):\n",
      "        return self.sum()\n",
      "\n",
      "#-------------- STATS NODES --------------------#\n",
      "    \n",
      "class StatsNode(Object):\n",
      "    \"\"\"\n",
      "    Abstract class for nodes which process statistics\n",
      "    \"\"\"\n",
      "    def results(self):\n",
      "        pass\n",
      "\n",
      "\n",
      "class LocateGapsNode(Node, StatsNode):\n",
      "    \n",
      "    postconditions = {} # Gaps will be identified!\n",
      "    \n",
      "    @staticmethod\n",
      "    def check_preconditions(conditions):\n",
      "        # Needs sample period to be set\n",
      "        # must be followed by a buffer node or an export node\n",
      "        \n",
      "class EnergyNode(Node, StatsNode):\n",
      "    \"\"\"Computes energy\"\"\"\n",
      "\n",
      "    name = 'energy'\n",
      "    \n",
      "    postconditions = {} # Energy is calculated\n",
      "\n",
      "    @staticmethod\n",
      "    def check_preconditions(conditions):\n",
      "        # Needs any power or energy measurements.  Preference is for\n",
      "        # energy measurements.  Computes energy for each of {active,\n",
      "        # reactive, apparent} available If only power measurements are\n",
      "        # available and if there are gaps then requires zeros to be\n",
      "        # inserted before gaps.\n",
      "           \n",
      "    def process(self, df):\n",
      "        # TODO: calculate energy_for_df\n",
      "        df.results[(self.name, self.instance)] = EnergyResults(energy_for_df)\n",
      "        return df\n",
      "    \n",
      "    def results(self):\n",
      "        return self._cumulator\n",
      "\n",
      "class ProportionEnergySubmeteredNode(Node, StatsNode):\n",
      "    def preconditions(self):\n",
      "        \"\"\"\n",
      "        * use the gaps in mains as a mask\n",
      "        * then calculate energy\n",
      "        \"\"\"\n",
      "\n",
      "#---------------- PROCESSING NODES --------------------#\n",
      "\n",
      "class ProcessingNode(Object):\n",
      "    \"\"\"\n",
      "    Abstract class for nodes which process data\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "class BookendGapsWithZerosNode(Node, ProcessingNode):\n",
      "    \n",
      "    postconditions = {} # Bookends gaps with zeros\n",
      "    \n",
      "    @staticmethod\n",
      "    def check_preconditions(conditions):\n",
      "        \"\"\"Requires gaps to be located.\"\"\"\n",
      "            \n",
      "    def process(self, df):\n",
      "        for gap_start, gap_end in df.metadata.gaps:\n",
      "            # insert zeros!\n",
      "            \n",
      "        return df\n",
      "    \n",
      "#--------------- EXPORT NODES --------------------#\n",
      "\n",
      "class ExportNode(Object):\n",
      "    \"\"\"\n",
      "    Abstract class for nodes which export data to disk / network etc\n",
      "    \"\"\"\n",
      "    pass\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's tinkering with some ideas for how the pipe will fit into the broader NILMTK framework.\n",
      "My hunch is that we should think of NILMTK in terms of layers of abstraction.  From the bottom up:\n",
      "\n",
      "--------------\n",
      "\n",
      "### Layer 1 (bottom layer): The physical layer\n",
      "\n",
      "* Deals with: retrieving data from disk / network / direct from a meter\n",
      "* Optimised for: handling large amounts of data\n",
      "* Services it provides: delivering a pd.DataFrame of data given a specific mask and columns\n",
      "* Totally agnostic about what the data 'means'.  It could be voltage, current, temperature, PIR readings etc.\n",
      "\n",
      "#### Classes in the Physical layer\n",
      "\n",
      "##### DataStore \n",
      "\n",
      "* loads a single chunk at a time from physical location and returns a DataFrame\n",
      "* subclasses for NILMTK HDF5, NILMTK CSV, Xively, REDD, iAWE, UKPD, etc; MetOffice XLS data, Current Cost meters etc.\n",
      "* One DataStore per HDF5 file or folder or CSV files or Xively feed etc\n",
      "* methods:\n",
      "  * __init__(start_date, end_date): start_date and end_date allow us to set a \"window of interest\", i.e. to crop the data.\n",
      "  * load(key, start_date, end_date): returns DataFrame, raises MemoryError if we try to load too much data.  key is the\n",
      "    location of a table.  There is assumed to be a one-to-one mapping from tables to meters. The hierarchical structure \n",
      "    of key is\n",
      "    standardised across NILMTK e.g. building1/utility/electric/meter4.  Or, if the physical store only represents,\n",
      "    say, a single house then the path is truncated to utility/electric/meter4.\n",
      "  * get_generator(key, periods): periods is a list of (start_date, end_date) tuples.\n",
      "  * copy(destination_datastore): this could be used, for example, to copy from REDD to NILMTK HDF5.\n",
      "* in the future, this could be responsible for efficiently compacting and uncompacting data (e.g. finding tables with common indicies and putting them into the same table; finding data which only has integer values and converting from float32 to int32 or even uint16)\n",
      "\n",
      "##### Loader\n",
      "\n",
      "* owns exactly one DataStore\n",
      "* knows how to chunk up data using masks\n",
      "* returns a generator\n",
      "* attributes\n",
      "  * key : string\n",
      "  * store : DataStore\n",
      "  * mask : Mask (basically a list of periods, i.e. a list of (start_date, end_date) tuples)\n",
      "  * period : string; a Pandas period alias e.g. 'D', 'M' etc\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "### Layer 2: The meter layer\n",
      "\n",
      "#### Classes in the meter layer\n",
      "\n",
      "##### Meter\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Meter(object):\n",
      "    \"\"\"\n",
      "    Attributes\n",
      "    ----------\n",
      "    loader : Loader\n",
      "    sample_period : float, seconds\n",
      "    max_sample_period : float, seconds\n",
      "    measurement_limits : dict.  e.g. {('power', 'active'): [0,3000], ('voltage', ''): [150,250]}\n",
      "    cleaning : dict of booleans, each defaults to True. \n",
      "       Controls whether data should be cleaned before returning answers.  e.g.\n",
      "       {'remove implausable values': True, 'bookend gaps with zeros': True}\n",
      "    \"\"\"\n",
      " \n",
      "    def power(self, measurement_preferences=None, required_measurement=None,\n",
      "              normalise=False, voltage=None):\n",
      "        \"\"\"Power timeseries.\n",
      "        \n",
      "        Set meter.loader parameters to configure chunk sizes, start date etc.\n",
      "        \n",
      "        The following cleaning steps will be run if the relevant entries\n",
      "        in meter.cleaning are True:\n",
      "\n",
      "        * remove implausable values\n",
      "        * gaps will be bookended with zeros\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        measurement_preferences : list of Measurements, optional. Defaults to active > apparent > reactive\n",
      "        required_measurements : Measurement, optional.  Raises MeasurementError if not available.\n",
      "        normalise : boolean, optional, defaults to False\n",
      "        voltage : Meter object with voltage measurements available.  If not supplied and if normalise is True\n",
      "            then will attempt to use voltage data from this meter.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        generator of pd.Series of power measurements.\n",
      "        \"\"\"\n",
      "        \n",
      "    def voltage(self):\n",
      "        \"\"\"Returns a generator of pd.Series of voltage, if avilable.\"\"\"\n",
      "        \n",
      "    def energy(self):\n",
      "        \"\"\"returns an EnergyResults object\"\"\"\n",
      "        nodes = [RemoveImplausableValues(self.measurement_limits), BookendGapsWithZeros(), Energy()]\n",
      "        pipeline = Pipeline(self.loader, nodes)\n",
      "        pipeline.run()\n",
      "        return pipline.results['energy']\n",
      "        \n",
      "    def dropout_rate(self):\n",
      "        \"\"\"returns a DropoutRateResults object.\"\"\"\n",
      "        \n",
      "    def gaps(self): \n",
      "        \"\"\"returns Mask object\"\"\"\n",
      "        \n",
      "    def clean_and_export(self, destination_datastore):\n",
      "        \"\"\"Apply all cleaning configured in meter.cleaning and then export.  Also identifies\n",
      "        and records the locations of gaps.  Also records metadata about exactly which\n",
      "        cleaning steps have been executed and some summary results (e.g. the number of\n",
      "        implausible values removed)\"\"\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# EXAMPLES:\n",
      "\n",
      "# To find total energy in kWh:\n",
      "meter.energy().combined\n",
      "\n",
      "# To find energy, after first masking out periods missing from the mains data:\n",
      "meter.loader.mask = mains_meter.gaps()\n",
      "meter.energy().combined\n",
      "\n",
      "# To find dropout rate, excluding gaps:\n",
      "meter.loader.mask = meter.gaps()\n",
      "meter.dropout_rate().combined\n",
      "\n",
      "# To find dropout rate and energy per hour:\n",
      "meter.loader.mask.clear()\n",
      "meter.loader.period = 'H'\n",
      "energy_per_period = meter.energy().per_period\n",
      "dropout_rate_per_period = meter.dropout_rate().per_period\n",
      "\n",
      "# Plot dropout rate per hour:\n",
      "meter.loader.period = 'H'\n",
      "meter.dropout_rate().per_period.plot()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "##### Pipeline\n",
      "Processes data from a single meter at a time.  Required so that we can run multiple chunks through the pipeline if the meter data is too large to fit into memory.  See above for details.\n",
      "\n",
      "##### Node (and all subclasses)\n",
      "See above for details\n",
      "\n",
      "##### Results (and all subclasses)\n",
      "See above for details\n",
      "  \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-----------\n",
      "\n",
      "### Layer 3: The mains and appliances layer\n",
      "\n",
      "There's a many-to-many mapping between appliance and meters:\n",
      "\n",
      "* sometimes one meter might measure multiple appliances\n",
      "* sometimes one appliance (e.g. Dual Supply) has multiple meters\n",
      "* but many times there is a one-to-one mapping between appliance and meter\n",
      "\n",
      "Also, there's a one-to-many mapping between 'mains feeds' and meters, \n",
      "either because there are multiple splits or phases; \n",
      "or because multiple meters measure the same mains split.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Mains(object):\n",
      "    def power(self, measurement_preferences=None, required_measurement=None,\n",
      "              normalise=False, voltage=None):\n",
      "        \"\"\"Power series.  Sums together three phases / dual split power.\"\"\"\n",
      "        \n",
      "    def energy(self):\n",
      "        \"\"\"Returns EnergyResults object???\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The building layer\n",
      "\n",
      "* The meter hierarchy is structured into a tree / graph\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The dataset layer\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}