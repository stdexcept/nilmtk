{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Pipeline(object):\n",
      "    \"\"\"\n",
      "    A data processing pipeline.\n",
      "      \n",
      "    A pipeline consists of exactly one data \"source\", which pulls data from disk / network / a device;\n",
      "    and an arbitrary number of \"nodes\" which process data in sequence or export the data to disk.\n",
      "    \n",
      "    Each processing node has a set of preconditions (e.g. gaps must be filled) and a set\n",
      "    of postconditions (e.g. gaps will have been filled).  This allows us to \n",
      "    check that a particular pipeline is viable (i.e. that, for every node,\n",
      "    the node's preconditions are satisfied by an upstream node or by the source).\n",
      "    \n",
      "    Internally, the pipeline is represented by a directed acyclic graphical model (DAG).\n",
      "    \n",
      "    DEVELOPMENT PLANS:\n",
      "    A pipeline could fork into multiple parallel pipelines.  Data and metadata would be copied to each fork\n",
      "    and each sub-pipeline would be run as a separate process (after checking requirements for each subpipeline as the start).\n",
      "    \n",
      "    Pipelines could be saved/loaded from disk; or rendered graphically.  In the future it would\n",
      "    be nice to have a full graphical UI (like Node-RED).\n",
      "    \n",
      "    Attributes\n",
      "    ----------\n",
      "    chain : networkx DAG containing all the processing nodes\n",
      "    source : Source\n",
      "    \"\"\"\n",
      "    def __init__(self):\n",
      "        self.instances = {}\n",
      "    \n",
      "    def insert_node(self, node):\n",
      "        # Insert node into the grahical model\n",
      "        node.instance = self.instances.get(node.name, 1)\n",
      "        self.instances[node.name] += 1\n",
      "    \n",
      "    def start(self):\n",
      "        # Reset\n",
      "        for node in self.nodes:\n",
      "            node.reset()\n",
      "        \n",
      "        # Check requirements\n",
      "        condition = self.source.condition()\n",
      "        # for example, `condition` might represent:\n",
      "        # * [measurements] are available\n",
      "        # * gaps = Gaps([(\"2013-01-01 00:00\", \"2013-01-01 00:10\"), ...])\n",
      "        # * zeros have been inserted\n",
      "        # * sample_period = 1 second (I was thinking of putting this in a separate, imutable 'metadata' field\n",
      "        #   but sample_period could be changed during the pipeline by an up/down-sampling node)\n",
      "        for node in self.nodes: # go through the nodes in order, starting upstream\n",
      "            unsatisfied_preconditions = condition.unsatisfied_preconditions(node.preconditions)\n",
      "            if unsatisfied_preconditions:\n",
      "                raise Exception(str(unsatisfied_preconsitions))\n",
      "                # TODO: should explain exactly why preconditions aren't satisfied\n",
      "                # e.g. which node is unsatisfied? Precisely which preconditions aren't met?\n",
      "            condition.update(node.postconditions)\n",
      "            # for example, `condition` might now include:\n",
      "            # * total energy for this chunk has been calculated\n",
      "            # * from [measurements] available, we must load reactive power and active power\n",
      "\n",
      "        # Load data and run processing\n",
      "        n_buffer_nodes = len([node for node in nodes if node.is_buffer])\n",
      "        for i in range(n_buffer_nodes):\n",
      "            for df in self.source.load(conditions.measurements()):\n",
      "                for node in nodes:\n",
      "                    df = node.process(df)\n",
      "                    if node.is_buffer and not node.is_full():\n",
      "                        break\n",
      "                # TODO: handle forks in pipeline???\n",
      "                \n",
      "            unfull_buffers = [node for node in nodes if node.is_buffer and not node.is_full()]\n",
      "            unfull_buffers[0].finish()\n",
      "                \n",
      "        # Do any tidying up required\n",
      "        for node in nodes:\n",
      "            node.finish()            \n",
      "\n",
      "class Source(object):\n",
      "    def load(self, measurements):\n",
      "        \"\"\"Returns a generator.  Each item returned by the generator is a pd.DataFrame with\n",
      "        metadata attached (sample period, gaps, gaps_bookended_with_zeros etc).\"\"\"\n",
      "    \n",
      "    def condition(self):\n",
      "        return Condition()\n",
      "        \n",
      "\n",
      "class Condition(object):\n",
      "    \"\"\"Stores pre-conditions and post-conditions.\n",
      "    \n",
      "    Requirements can be of the form:\n",
      "    \n",
      "    \"node X needs (power.apparent or power.active) (but not power.reactive)\n",
      "    and voltage is useful but not essential\" \n",
      "    \n",
      "    or\n",
      "    \n",
      "    \"node Y needs everything available from disk (to save to a copy to disk)\"\n",
      "    \n",
      "    or\n",
      "    \n",
      "    \"ComputeEnergy node needs gaps to be bookended with zeros\" (if none of the previous nodes\n",
      "    provide this service then check source.metadata to see if zeros have already been inserted;\n",
      "    if the haven't then raise an error to tell the user to add a BookendGapsWithZeros node.)\n",
      "    \"\"\"\n",
      "    def update(self, postconditions):\n",
      "        \"\"\"Add postconditions to self.\"\"\"\n",
      "        pass\n",
      "    \n",
      "    def unsatisfied_preconditions(self, preconditions):\n",
      "        \"\"\"Returns list of preconditions not satisfied by self.\"\"\"\n",
      "        \n",
      "\n",
      "class Node(object):\n",
      "    \"\"\"\n",
      "    Abstract class defining interface for all Node subclasses, \n",
      "    where a 'node' is a module which runs pre-processing or statistics or NILM training or disaggregation.\n",
      "    \"\"\"\n",
      "    def __init__(self):\n",
      "\n",
      "    @classmethod\n",
      "    def preconditions(self):\n",
      "        return preconditions\n",
      "    \n",
      "    @classmethod\n",
      "    def postconditions(self):\n",
      "        return postconditions\n",
      "    \n",
      "    def process(self, df):\n",
      "        # do stuff to df\n",
      "        return df\n",
      "    \n",
      "#-------------- RESULT CLASSES -----------------#\n",
      "\n",
      "\"\"\"Metadata results from each node need to be assigned to a specific class\n",
      "so we know how to combine results from multiple chunks.  For example,\n",
      "Energy can be simply summed; while dropout rate should be averaged,\n",
      "and gaps need to be merged across chunk boundaries.\n",
      "Results are a subclass of DataFrame.  The index is the start timestamp\n",
      "for which the results are valid; the first column ('end') is the end timestamp\n",
      "for which the results are valid.  Other columns are the results.\n",
      "\"\"\"\n",
      "\n",
      "class Results(pd.DataFrame):\n",
      "    def combined(self):\n",
      "        \"\"\"Return all results from each chunk combined\"\"\"\n",
      "\n",
      "class EnergyResults(Results):\n",
      "    def combined(self):\n",
      "        return self.sum()\n",
      "\n",
      "#-------------- STATS NODES --------------------#\n",
      "    \n",
      "class StatsNode(Object):\n",
      "    \"\"\"\n",
      "    Abstract class for nodes which process statistics\n",
      "    \"\"\"\n",
      "    def results(self):\n",
      "        pass\n",
      "\n",
      "\n",
      "class LocateGapsNode(Node, StatsNode):\n",
      "    \n",
      "    @classmethod\n",
      "    def preconditions(self):\n",
      "        # Needs sample period to be set\n",
      "        # must be followed by a buffer node or an export node\n",
      "        \n",
      "    @classmethod\n",
      "    def postconditions(self):\n",
      "        # Gaps will be identified!\n",
      "\n",
      "class EnergyNode(Node, StatsNode):\n",
      "    \"\"\"Computes energy\"\"\"\n",
      "\n",
      "    name = 'energy'\n",
      "\n",
      "    @classmethod\n",
      "    def preconditions(self):\n",
      "        # Needs any power or energy measurements.  Preference is for energy measurements.\n",
      "        # Computes energy for each of {active, reactive, apparent} available\n",
      "        # If only power measurements are available and if there are gaps then requires\n",
      "        # zeros to be inserted before gaps.\n",
      "        \n",
      "    @classmethod\n",
      "    def postconditions(self):\n",
      "        # Energy is calculated\n",
      "        pass\n",
      "           \n",
      "    def process(self, df):\n",
      "        # TODO: calculate energy_for_df\n",
      "        df.metadata[(self.name, self.instance)] = EnergyResults(energy_for_df)\n",
      "        return df\n",
      "    \n",
      "    def results(self):\n",
      "        return self._cumulator\n",
      "\n",
      "class ProportionEnergySubmeteredNode(Node, StatsNode):\n",
      "    def preconditions(self):\n",
      "        \"\"\"\n",
      "        * use the gaps in mains as a mask\n",
      "        * then calculate energy\n",
      "        \"\"\"\n",
      "\n",
      "#---------------- PROCESSING NODES --------------------#\n",
      "\n",
      "class ProcessingNode(Object):\n",
      "    \"\"\"\n",
      "    Abstract class for nodes which process data\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "class BookendGapsWithZerosNode(Node, ProcessingNode):\n",
      "    \n",
      "    def preconditions(self):\n",
      "        \"\"\"Requires gaps to be located.\"\"\"\n",
      "        \n",
      "    def postconditions(self):\n",
      "        \"\"\"Bookends gaps with zeros\"\"\"\n",
      "    \n",
      "    def process(self, df):\n",
      "        for gap_start, gap_end in df.metadata.gaps:\n",
      "            # insert zeros!\n",
      "            \n",
      "        return df\n",
      "    \n",
      "#--------------- EXPORT NODES --------------------#\n",
      "\n",
      "class ExportNode(Object):\n",
      "    \"\"\"\n",
      "    Abstract class for nodes which export data to disk / network etc\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "#--------------- BUFFER NODES ---------------------#\n",
      "\n",
      "class BufferNode(Node):\n",
      "    \"\"\"BufferNodes solve the following problem. Say we need to run the chain on a dataset\n",
      "    too large to fit in memory and we have a processing chain like this:\n",
      "    \n",
      "    Source -> find gaps -> split on gaps -> calculate dropout rate\n",
      "    \n",
      "    In this case, we only have full knowledge of where the gaps are once we have looked at\n",
      "    all chunks.  A BufferNode forces the Pipeline to run all chunks through the pipeline\n",
      "    up to the Buffer (and \"finish\" all metadata) before proceeding.\n",
      "    \n",
      "    Source -> find gaps -> BufferNode -> split on gaps -> calculate dropout rate\n",
      "    \n",
      "    BufferNode passed just the metadata onto the next stage.\n",
      "    \n",
      "    QUESTION: BufferNodes will always be proceeded by splitter nodes so maybe they should\n",
      "    be combined?\n",
      "    \n",
      "    QUESTION: Splitter nodes will need direct access to the Source.  How to implement this?\n",
      "    Maybe pass in a ref to Source to all Nodes???\n",
      "    \n",
      "    QUESTION: maybe this is over complex???  Maybe we should force users to first run\n",
      "    \n",
      "    Source('a.h5') -> find gaps -> export('a.h5')\n",
      "    \n",
      "    And then do\n",
      "    \n",
      "    Source('a.h5') -> split on gaps -> ...\n",
      "    \"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}